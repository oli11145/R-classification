---
title: "Classification Project"
subtitle: "Machine Learning 2"
author: "Aleksandra Jendrusiak, Weronika Motkowska"
output:
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
    smooth_scroll: true
editor_options: 
  chunk_output_type: console
---
  
  ```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
  ```
The aim of the project is to forecast whether there will be rain tomorrow in Australia. Weather forecasting is a part of our everyday life and especially rainfall and other precipitation might influence our plans for the next day. The data is available on Kaggle.com. 

First, we install all necessary packages. 
```{r}
# install.packages("tidyverse")
# install.packages("tidygeocoder")
# install.packages("lubridate")
# install.packages("hydroTSM")
# install.packages("mltools") # one_hot (replace by caret?)
# install.packages("ozmaps")
# install.packages("tree")
# install.packages("rpart")
# install.packages("rpart.plot")
# install.packages("rattle")
# install.packages("here")
# install.packages("caret")
# install.packages("randomForest")
# install.pacages("gbm")
```

Next, we load them into memory. 
```{r message = F, warning = F}
library(dplyr)
library(ggplot2)
library(data.table) 
library(ggpubr)
library(tidyverse)
library(tidygeocoder)
library(lubridate)
library(hydroTSM)
library(mltools)
library(ozmaps)
library(tree)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(pROC)
library(here)
library(randomForest)
library(gbm)
```

# Data loading

At the beginning, we can load our data. We have 30 varaibles of different types: continuous, discrete, categorical.
```{r}
data <- read.csv(here("data", "weatherAUS.csv")) %>% as.data.frame()
glimpse(data)
summary(data)
```

# Data visualization

First, we can create the bar plot of the *target variable* which is *RainTomorrow*.
```{r message = F, warning = F}
ggplot(data = data, aes(x = factor(RainTomorrow))) +
  geom_bar(stat = "count", fill = "steelblue") +
  labs(title = "Barplot of RainTomorrow", 
         x = "RainTomorrow", y = "Count") +
  theme_minimal()
```

Now we can also visualize the explanatory variables. First we create barplots for discrete variables.
```{r message = F, warning = F}
discrete <- c("RainToday", "WindGustDir", "WindDir9am", "WindDir3pm", "Cloud9am", "Cloud3pm")

plots_dicrete <- lapply(
              discrete,
              function(variable) {
                x <- ggplot(data, aes(x = eval(parse(text = variable)))) +
                         geom_bar(stat = "count", fill = "steelblue") + 
                  labs(title = paste("Barplot of", variable), 
                       x = variable, y = "Count") + 
                  theme_minimal()
              }
)

allplots <- ggarrange(plotlist = plots_dicrete,
                      ncol = 2, nrow = 3)

allplots
```

Now we create histograms of continuous varibles.
```{r message = F, warning = F}
continuous <- colnames(data %>% select(-c(all_of(discrete), "Date", "Location", "RainTomorrow")))

plots_continuous <- lapply(
              continuous,
              function(variable) {
                x <- ggplot(data, aes(x = eval(parse(text = variable)))) +
                         geom_histogram(aes(y = ..density..),  fill = "steelblue") +
                  labs(title = paste("Histogram of", variable), 
                       x = variable, y = "Density") + 
                  geom_density() +  
                  theme_minimal()
              }
)

allplots <- ggarrange(plotlist = plots_continuous,
                      ncol = 3, nrow = 5)

allplots
```

# Data transformation

First, we need to transform the *target variable* into binary number.
```{r}
data$RainTomorrow <- ifelse(data$RainTomorrow == "Yes", 1, 0)
```

Some of the varibles such like *Rainfall* or *Location* might bring some information about the target variable, but might not be useful in a straightfroward way (the distribution is skewed and with many outliers), so we will transform them. We will also transform the *Location* into *Latitude* and *Longitude* and extract some infromation from the *Date* variable.
```{r}
data$RainfallBin <- ifelse(data$Rainfall > 0, 1, 0)
data$RainfallLog <- log(data$Rainfall + 1)

# capping using inter quantile range
Rainfall.Q25 <- quantile(data$Rainfall, 0.25, na.rm = TRUE)
Rainfall.Q75 <- quantile(data$Rainfall, 0.75, na.rm = TRUE)
Rainfall.IQR <-  Rainfall.Q75 - Rainfall.Q25
data$RainfallCap <- data$Rainfall
data$RainfallCap[which(data$RainfallCap > Rainfall.Q75 + 3 * Rainfall.IQR)] <- Rainfall.Q75 + 3 * Rainfall.IQR

data$EvaporationBin <- ifelse(data$Evaporation > 0, 1, 0)
data$EvaporationLog <- log(data$Evaporation + 1)

# capping using inter quantile range
Evaporation.Q25 <- quantile(data$Evaporation, 0.25, na.rm = TRUE)
Evaporation.Q75 <- quantile(data$Evaporation, 0.75, na.rm = TRUE)
Evaporation.IQR <-  Evaporation.Q75 - Evaporation.Q25
data$EvaporationCap <- data$Evaporation
data$EvaporationCap[which(data$EvaporationCap > Evaporation.Q75 + 3 * Evaporation.IQR)] <- Evaporation.Q75 + 3 * Evaporation.IQR

data$RainToday <- ifelse(data$RainToday == "Yes", 1, 0)

# converting Location into Lon gitude and Latitude
data$Location <- gsub("([a-z])([A-Z])", "\\1 \\2", data$Location)
data <- geocode(data, city = Location, method = "osm") %>%
  rename("Latitude" = "lat", "Longitude" = "long")
ggplot(data, aes(x = Longitude, y = Latitude, color = RainTomorrow)) + geom_point()

# extracting information from Date
data$Date <- as.Date(data$Date, "%Y-%m-%d")
data$Season <- time2season(data$Date, out.fmt = "seasons") 
# data$Year <- year(data$Date)
# data$Month <- month(data$Date)
# data$Day <- day(data$Date)

# reducing the wind directions (from 16 to 8)
for (var in c("WindGustDir", "WindDir9am", "WindDir3pm")){
  data[, var] <- substr(data[, var][[1]], 1, 2)
}

continuous <- c(continuous, "RainfallLog", "EvaporationLog", "RainfallCap", "EvaporationCap", "Longitude", "Latitude")
discrete <- c(discrete, "RainfallBin", "EvaporationBin", "Season", "Location")
```

Now, we will split the data into the train and test set. 
```{r}
data <- data[!is.na(data$RainTomorrow), ] # removing NAs in target variable

set.seed(123456789)
training_obs <- createDataPartition(data$RainTomorrow,
                                    p = 0.7,
                                    list = FALSE)

data.train <- data[training_obs, ] %>% as.data.frame()
data.test  <- data[-training_obs, ] %>% as.data.frame()
```

# Correlation analysis

Now we can analyze the correlation between variables.
```{r}
numeric.train <- data.train[, c(continuous, "RainTomorrow", "RainToday", "EvaporationBin", "RainfallBin")]

data.train.cor.pearson = cor(na.omit(numeric.train), method = "pearson")
data.train.cor.pearson[, "RainTomorrow"] %>% sort()

data.train.cor.spearman = cor(na.omit(numeric.train), method = "spearman")
data.train.cor.spearman[, "RainTomorrow"] %>% sort() 
```

We can remove the binned and capped variables as their prediction power is lower than original variables. Also we will leave logarithm instead of standard variable to have less skewed distribution.
```{r}
data.train <- data.train %>% select(-c(Evaporation, EvaporationBin, Rainfall, RainfallBin, EvaporationCap, RainfallCap))
data.test <- data.test %>% select(-c(Evaporation, EvaporationBin, Rainfall, RainfallBin, EvaporationCap, RainfallCap))

continuous <- continuous[!continuous %in% c("Evaporation", "Rainfall", "EvaporationCap", "RainfallCap")]
discrete <- discrete[!discrete %in% c("EvaporationBin", "RainfallBin")]
```

# Missing values

Before starting buliding model we should handle the missing values. First, check how many NAs each varaible has.
```{r}
missing.percentage <- round((colMeans(is.na(data.train))) * 100, 2)
missing.percentage %>% sort(decreasing = TRUE) 
```

For the other variables we will impute the median (for continuous variables) or the mode (for discrete variables) in the train dataset.
```{r}
# removing NAs in continuous variables by median
for (variable in continuous){
  data.train[is.na(data.train[, variable]), variable] = median(na.omit(data.train[, variable])) 
}

# removing NAs in discrete variables by mode
Mode <- function(x) {
  unique.x <- unique(x)
  return(unique.x[which.max(tabulate(match(x, unique.x)))])
}

for (variable in discrete){
  data.train[is.na(data.train[, variable]), variable] = Mode(na.omit(data.train[, variable])) 
}

round((colMeans(is.na(data.train))) * 100, 2) # all zeros
```

We should also handle the missing values in the test dataset.
```{r}
# removing NAs in continuous variables by median
for (variable in continuous){
  data.test[is.na(data.test[, variable]), variable] = median(na.omit(data.test[, variable])) 
}

# removing NAs in discrete variables by mode
for (variable in discrete){
  data.test[is.na(data.test[, variable]), variable] = Mode(na.omit(data.test[, variable]))
}

round((colMeans(is.na(data.test))) * 100, 2) # all zeros
```

# Feature scaling

As most ML models perform poorly with variables on different scale, we can now standardize all numerical variables.
```{r}
(train.maxs <- apply(data.train %>% select(continuous), 2, max))
(train.mins <- apply(data.train %>% select(continuous), 2, min))

data.train <- data.train %>%
  mutate(as.data.frame(scale(data.train %>% select(continuous), 
                             center = train.mins, 
                             scale  = train.maxs - train.mins)))

data.test <- data.test %>%
  mutate(as.data.frame(scale(data.test %>% select(continuous), 
                             center = train.mins, 
                             scale  = train.maxs - train.mins)))
```

We have also some categorical variables that we need to transform.
```{r}
categorical <- c("WindGustDir", "WindDir3pm", "WindDir9am", "Season", "RainToday")

data.train <- as.data.frame(unclass(data.train), stringsAsFactors = TRUE) %>% select(-c(Location, Date))
data.train <- one_hot(as.data.table(data.train), cols = categorical, dropUnusedLevels = TRUE)

data.test <- as.data.frame(unclass(data.test), stringsAsFactors = TRUE) %>% select(-c(Location, Date))
data.test <- one_hot(as.data.table(data.test), cols = categorical, dropUnusedLevels = TRUE)
```

The data is ready for the modeling.
```{r}
summary(data.train)
```

Both sets will be saved as the `*.rds` files.
```{r}
data.train %>% saveRDS(here("data", "data.train.rds"))
data.test  %>% saveRDS(here("data", "data.test.rds"))
```

# Logistic Regression

Our first model will be a classical logistic regression on the basis of all predictors. Firstly we will define the model formula.  
```{r}
target <- "RainTomorrow"
variables <- colnames(data.train)[colnames(data.train) != target]

formula <- paste(target, paste(variables, collapse = ' + '), sep = " ~ ")
```

Now we can fit the logistic regression model.
```{r}
logistic <- glm(formula, 
                data = data.train, 
                family = "binomial")
logistic
summary(logistic)

saveRDS(object = logistic,
        file = here("output", "logistic.rds"))
```

We can evaluate the ROC AUC and plot the ROC curve.
```{r}
pred.train.log <- predict(logistic,
                          data.train, type = "response")
ROC.train.log <- roc(data.train$RainTomorrow, 
                        as.numeric(pred.train.log))

ROC.train.log %>%
pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
              color = "grey", 
              linetype = "dashed") +
labs(subtitle = paste0("ROC AUC TRAIN: ",
                        "logistic = ", 
                        round(100 * auc(ROC.train.log) , 1), "%, ")) +
theme_bw() + coord_fixed()
```

We can also look at the confusion matrix.
```{r}
pred.train.log.class <- ifelse(pred.train.log > 0.5, 1, 0) # different cutoff?

confusionMatrix(data = as.factor(pred.train.log.class),
                reference = as.factor(data.train$RainTomorrow),
                positive = "1") 
```

# Decision Trees

We can create the first tree on the basis of all meaningful predictors. The default splitting criterion is the Gini Index.
```{r}
tree1 <- 
  rpart(formula,
        data = data.train,
        method = "class")

saveRDS(object = tree1,
        file = here("output", "tree1.rds"))
```

Examine and plot the tree.
```{r}
tree1
summary(tree1)

rpart.plot(tree1)
fancyRpartPlot(tree1)
```

We can also try to build a tree based on the entropy and compare it with the tree based on the Gini Index.
```{r}
tree2 <- 
  rpart(formula,
        data = data.train,
        method = "class",
        parms = list(split = 'information'))

saveRDS(object = tree2,
        file = here("output", "tree2.rds"))

tree2
summary(tree2)

rpart.plot(tree2)
fancyRpartPlot(tree2)
```

We can try to build a tree with lower restrictions.
```{r}
tree3 <- 
  rpart(formula,
        data = data.train,
        method = "class",
        minsplit = 2000, # ~ 2% of the training set
        minbucket = 1000, # ~ 1% of the training set
        maxdepth = 30, # default
        cp = -1)
fancyRpartPlot(tree3)

saveRDS(object = tree3,
        file = here("output", "tree3.rds"))
```

Let's find and save number of the `cp` row with the lowest error.
```{r}
opt <- which.min(tree3$cptable[, "xerror"])
cp <- tree3$cptable[opt, "CP"]
plotcp(tree3)
```

Now we can prune the 4th tree.
```{r}
tree3p <- 
  prune(tree3, cp = cp)
fancyRpartPlot(tree3p)

saveRDS(object = tree3p,
        file = here("output", "tree3p.rds"))
```

We can make a prediction based on the fitted trees and evaluate the errors.
```{r}
pred.train.tree1 <- predict(tree1, data.train)
pred.train.tree2 <- predict(tree2, data.train)
pred.train.tree3 <- predict(tree3, data.train)
pred.train.tree3p <- predict(tree3p, data.train)

ROC.train.tree1 <- roc(data.train$RainTomorrow == "1", 
                        pred.train.tree1[, 2])
ROC.train.tree2 <- roc(data.train$RainTomorrow == "1", 
                        pred.train.tree2[, 2])
ROC.train.tree3 <- roc(data.train$RainTomorrow == "1", 
                        pred.train.tree3[, 2])
ROC.train.tree3p <- roc(data.train$RainTomorrow == "1", 
                        pred.train.tree3p[, 2])
```

We can plot the ROC curves.
```{r}
list(
  ROC.train.tree1  = ROC.train.tree1,
  ROC.train.tree3  = ROC.train.tree3,
  ROC.train.tree3p = ROC.train.tree3p
) %>%
  ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "tree1 = ", 
                         round(100*(2 * auc(ROC.train.tree1) - 1), 1), "%, ",
                         "tree3 = ", 
                         round(100*(2 * auc(ROC.train.tree3) - 1), 1), "%, ",
                         "tree3p = ", 
                         round(100*(2 * auc(ROC.train.tree3p) - 1), 1), "% ")) +
  theme_bw() + coord_fixed() + theme(text = element_text(size = 18)) 
```

We can also look at the confusion matrix.
```{r}
pred.train.tree3.class <- predict(tree3, data.train, type = "class")
confusionMatrix(data = pred.train.tree3.class,
                reference = as.factor(data.train$RainTomorrow),
                positive = "1") 

pred.train.tree3p.class <- predict(tree3p, data.train, type = "class")
confusionMatrix(data = pred.train.tree3p.class,
                reference = as.factor(data.train$RainTomorrow),
                positive = "1") 
```

We can look at the importance of the variables.
```{r}
options(scipen = 100)
tree3.importance <- tree3$variable.importance
tree3.importance %>% sort(decreasing = TRUE)
```

# Random Forest

First, we need to transform target value to factors.
```{r}
data.train$RainTomorrow <- as.factor(data.train$RainTomorrow)
```

Now we can evaluate the Random Forest classifier.
```{r}
set.seed(123456789)
random.forest1 <- randomForest(as.formula(formula),
                              data = data.train)

saveRDS(object = random.forest1,
        file = here("output", "random.forest1.rds"))
```

We can examine the results and create a plot.
```{r}
print(random.forest1)
plot(random.forest1)
```

We can experiment with setting different number of trees or using more variables.
```{r}
random.forest2 <- 
  randomForest(as.formula(formula),
               data = data.train,
               ntree = 300,
               sampsize = nrow(data.train),
               mtry = 8,
               # minimum number of obs in the terminal nodes
               nodesize = 100,
               # we also generate predictors importance measures
               importance = TRUE)
print(random.forest2)

saveRDS(object = random.forest2,
        file = here("output", "random.forest2.rds"))
```

Let's find the best value of the number of variables (*mtry*) using cross validation. As it is very time consuming it was performed once and the results were saved in the *.rds* file.
```{r}
if(0){
  set.seed(123456789)
  
  parameters_rf <- expand.grid(mtry = 2:30)
  ctrl_cv <- trainControl(method = "cv",
                        number = 5,
                        classProbs = TRUE)
  
  # we need to change the level names of the target variable as 0 and 1 produce error in the train function
  data.train2 <- data.train %>%
     mutate(RainTomorrow = factor(RainTomorrow, 
                                  labels = c("No", "Yes")))
  
  random.forest3 <-
  train(as.formula(formula),
        data = data.train2,
        method = "rf",
        ntree = 100,
        nodesize = 100,
        tuneGrid = parameters_rf,
        trControl = ctrl_cv,
        importance = TRUE)
  
  saveRDS(object = random.forest3,
        file = here("output", "random.forest3.rds"))
} # very time consuming!

random.forest3 <- readRDS(file = here("output", "random.forest3.rds"))
```

Let's look at the plot.
```{r}
random.forest3
str(random.forest3)
plot(random.forest3$results$mtry,
     random.forest3$results$Accuracy, type = "b")
plot(random.forest3$results$mtry,
     random.forest3$results$Kappa, type = "b")
```

Looks like optimal value is around `mtry = 12` (probably might be higher, but computational time would be also longer).
```{r}
# Model with mtry = 12
random.forest4 <- 
  randomForest(as.formula(formula),
               data = data.train,
               ntree = 300,
               sampsize = nrow(data.train),
               mtry = 12,
               nodesize = 100,
               importance = TRUE)

saveRDS(object = random.forest4,
      file = here("output", "random.forest4.rds"))
```

Let's plot the ROC curves.
```{r}
pred.train.rf1 <- predict(random.forest1, 
                         data.train, 
                         type = "prob")[, "1"]
pred.train.rf2 <- predict(random.forest2, 
                         data.train, 
                         type = "prob")[, "1"]
pred.train.rf4 <- predict(random.forest4, 
                         data.train, 
                         type = "prob")[, "1"]

ROC.train.rf1  <- roc(data.train$RainTomorrow == "1",
                      pred.train.rf1)
ROC.train.rf2  <- roc(data.train$RainTomorrow == "1",
                      pred.train.rf2)
ROC.train.rf4  <- roc(data.train$RainTomorrow == "1",
                      pred.train.rf4)

list(
  ROC.train.rf1 = ROC.train.rf1,
  ROC.train.rf2 = ROC.train.rf2,
  ROC.train.rf4 = ROC.train.rf4
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
               color = "grey",
               linetype = "dashed") +
  labs(title = paste0("Gini TRAIN: ",
                      "rf1 = ",
                      round(100 * (2 * auc(ROC.train.rf1) - 1), 1), "%, ",
                      "rf2 = ",
                      round(100 * (2 * auc(ROC.train.rf2) - 1), 1), "%, ",
                      "rf4 = ",
                      round(100 * (2 * auc(ROC.train.rf4) - 1), 1), "%, ")) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")

```

We can analyze the variables importance.
```{r}
varImpPlot(random.forest3$finalModel,
           sort = TRUE,
           main = "Importance of predicors",
           n.var = 10,
           type = 1) # mean decrease in accuracy

varImpPlot(random.forest3$finalModel,
           sort = TRUE,
           main = "Importance of predictors",
           n.var = 10,
           type = 2) # mean decrease in node impurity

```

# Gradient Boosting

Let's create the first gradient boosting model with arbitrarly chosen parameters.
```{r}
# gbm does not work with a factor variables
data.train$RainTomorrow <- as.character(data.train$RainTomorrow)

start <- Sys.time()
gbm1 <- 
  gbm(as.formula(formula),
      data = data.train,
      distribution = "bernoulli",
      # total number of trees
      n.trees = 500,
      # number of variable interactions - actually depth of the trees
      interaction.depth = 4,
      # shrinkage parameter - speed (pace) of learning
      shrinkage = 0.01,
      verbose = FALSE)
end <- Sys.time()
print(end - start)

gbm1 %>% saveRDS(here("output", "gbm1.rds"))
```

We can perform the parameters tuning (should take around 40min, so it was performed once and results were saved).
```{r}
modelLookup("gbm")
parameters_gbm <- expand.grid(interaction.depth = c(2, 4),
                             n.trees = c(200, 500),
                             shrinkage = c(0.01, 0.1), 
                             n.minobsinnode = c(200, 500))
ctrl_cv3 <- trainControl(method = "cv", 
                         number = 3,
                         classProbs = TRUE,
                         summaryFunction = twoClassSummary)

if (0) {
  set.seed(123456789)
  
  # we need to change the level names of the target variable as 0 and 1 produce error in the train function
  data.train2 <- data.train %>%
     mutate(RainTomorrow = factor(RainTomorrow, 
                                  labels = c("No", "Yes")))
  
  gbm2  <- train(as.formula(formula),
                 data = data.train2,
                 distribution = "bernoulli",
                 method = "gbm",
                 tuneGrid = parameters_gbm,
                 trControl = ctrl_cv3,
                 verbose = FALSE)
  
  saveRDS(object = gbm2,
          file = here("output", "gbm2.rds"))
}

gbm2 <- readRDS(here("output", "gbm2.rds"))
gbm2
```

Let's look at the ROC on the train dataset for both models.
```{r}
pred.train.gbm1 <- predict(gbm1, 
                           data.train, 
                           type = "response",
                           ntrees = 500)
pred.train.gbm2 <- predict(gbm2, 
                           data.train, 
                           type = "prob",
                           ntrees = 500)[, "Yes"]

ROC.train.gbm1  <- roc(data.train$RainTomorrow == "1",
                       pred.train.gbm1)
ROC.train.gbm2  <- roc(data.train$RainTomorrow == "1",
                       pred.train.gbm2)

list(
  ROC.train.gbm1 = ROC.train.gbm1,
  ROC.train.gbm2 = ROC.train.gbm2
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "gbm1 = ", 
                         round(100 * (2 * auc(ROC.train.gbm1) - 1), 1), "%, ",
                         "gbm2 = ", 
                         round(100 * (2 * auc(ROC.train.gbm2) - 1), 1), "%, "
  )) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```

# Comparison of models

We can plot ROC curves on the test data set.
```{r}
pred.test.log <- predict(logistic,
                         data.test, 
                         type = "response")
pred.test.tree3p <- predict(tree3p, 
                            data.test)
pred.test.rf4 <- predict(random.forest4, 
                         data.test, 
                         type = "prob")[, "1"]
pred.test.gbm2 <- predict(gbm2, 
                          data.test, 
                          type = "prob",
                          ntrees = 500)[, "Yes"]

ROC.test.log <- roc(data.test$RainTomorrow, 
                        as.numeric(pred.test.log))
ROC.test.tree3p <- roc(data.test$RainTomorrow == "1", 
                        pred.test.tree3p[, 2])
ROC.test.rf4  <- roc(data.test$RainTomorrow == "1",
                      pred.test.rf4)
ROC.test.gbm2  <- roc(data.test$RainTomorrow == "1",
                       pred.test.gbm2)

list(
  ROC.test.log = ROC.test.log,
  ROC.test.tree3p = ROC.test.tree3p,
  ROC.test.rf4 = ROC.test.rf4,
  ROC.test.gbm2 = ROC.test.gbm2
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
               color = "grey",
               linetype = "dashed") +
  labs(title = paste0("ROC AUC test: ",
                      "logistic = ",
                      round(100 * auc(ROC.test.log) , 1), "%, ",
                      "tree = ",
                      round(100 * auc(ROC.test.tree3p) , 1), "%, ",
                      "rf = ",
                      round(100 * auc(ROC.test.rf4) , 1), "%, ",
                      "gbm = ",
                      round(100 * auc(ROC.test.gbm2) , 1), "%, ")) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired") + theme(text = element_text(size = 12))
```

Confusion matrix for the best model.
```{r}
pred.test.gbm2.class <- predict(gbm2, data.test, type = "raw")
confusionMatrix(data = pred.test.gbm2.class,
                reference = factor(data.test$RainTomorrow, labels = c("No", "Yes")),
                positive = "Yes") 
```

# Ensembling

We can try to mix the models in order to obtain even better fit.
```{r}
models <- c("logistic", "rf4", "gbm2")
preds.train <- data.frame("logistic" = pred.train.log, "rf4" = pred.train.rf4, "gbm2" = pred.train.gbm2)

cor(preds.train)
corrplot::corrplot(cor(preds.train))
```

Correlations are quite high, but still we can try to ensemble and compare results. We can try majority voting.
```{r}
preds.train$major.voting <-
  ifelse(rowSums(preds.train[, models] > 0.5) > 1,
         1, 
         0)
```

And also simple averaging.
```{r}
preds.train$simple.averaging <-
  ifelse(rowMeans(preds.train[, models]) > 0.5,
         1, 
         0)
```

Let's plot the ensemble models on the train data set.
```{r}
ROC.train.m.voting <- roc(data.train$RainTomorrow, 
                          preds.train$major.voting)
ROC.train.s.avg <- roc(data.train$RainTomorrow, 
                      preds.train$simple.averaging)

list(
  ROC.train.m.voting = ROC.train.m.voting,
  ROC.train.s.avg = ROC.train.s.avg
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
               color = "grey",
               linetype = "dashed") +
  labs(title = paste0("ROC AUC test: ",
                      "majority voting = ",
                      round(100 * auc(ROC.train.m.voting) , 1), "%, ",
                      "simple averaging = ",
                      round(100 * auc(ROC.train.s.avg) , 1), "%, ")) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired") + theme(text = element_text(size = 12))
```

And on the test data set.
```{r}
preds.test <- data.frame("logistic" = pred.test.log, "rf4" = pred.test.rf4, "gbm2" = pred.test.gbm2)

preds.test$major.voting <-
  ifelse(rowSums(preds.test[, models] > 0.5) > 1,
         1, 
         0)

preds.test$simple.averaging <-
  ifelse(rowMeans(preds.test[, models]) > 0.5,
         1, 
         0)

ROC.test.m.voting <- roc(data.test$RainTomorrow, 
                          preds.test$major.voting)
ROC.test.s.avg <- roc(data.test$RainTomorrow, 
                      preds.test$simple.averaging)

list(
  ROC.test.m.voting = ROC.test.m.voting,
  ROC.test.s.avg = ROC.test.s.avg
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
               color = "grey",
               linetype = "dashed") +
  labs(title = paste0("ROC AUC test: ",
                      "majority voting = ",
                      round(100 * auc(ROC.test.m.voting) , 1), "%, ",
                      "simple averaging = ",
                      round(100 * auc(ROC.test.s.avg) , 1), "%, ")) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired") + theme(text = element_text(size = 12))
```

Ensembling did not improve the performance. Overall, the best model is gradient boosting with the ROC AUC score equal to 88.5